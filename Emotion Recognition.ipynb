{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition :\n",
    "> Dataset obtained from kaggle. It contains gray-scale images of different human emotions.\n",
    "\n",
    "### Task -\n",
    "\n",
    "> Develope an app which will predict human emotion using webcam\n",
    "\n",
    "\n",
    "### Importing Liraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\tqdm\\autonotebook\\__init__.py:14: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import models\n",
    "from torch.autograd import Variable\n",
    "from random import randint\n",
    "from tqdm.autonotebook import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "plt.ion()   # interactive mode\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Emotion</th>\n",
       "      <th>Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>221 240 251 254 255 255 255 255 255 255 255 25...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>100 107 108 104 103 113 117 115 120 130 138 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>35 50 56 57 63 76 74 79 85 86 105 133 145 152 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>119 124 129 135 136 140 142 149 159 156 163 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>160 173 186 194 188 185 175 162 153 143 135 12...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Emotion                                             Pixels\n",
       "0        3  221 240 251 254 255 255 255 255 255 255 255 25...\n",
       "1        6  100 107 108 104 103 113 117 115 120 130 138 14...\n",
       "2        4  35 50 56 57 63 76 74 79 85 86 105 133 145 152 ...\n",
       "3        6  119 124 129 135 136 140 142 149 159 156 163 16...\n",
       "4        2  160 173 186 194 188 185 175 162 153 143 135 12..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "em = np.array(data['Emotion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14291263762565848\n"
     ]
    }
   ],
   "source": [
    "avg_error = 0\n",
    "for i in range(1000):\n",
    "    rpred = [randint(0,6) for i in range(len(em))]\n",
    "    x = em == rpred\n",
    "    x = sum(x)\n",
    "    avg_error += x*1.0/len(em)\n",
    "\n",
    "print(avg_error/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string of pixels to list\n",
    "def changeTypeToNumpyArray(x):\n",
    "    x = x.split()\n",
    "    x = [int(i) for i in x]\n",
    "    return x\n",
    "\n",
    "# Reshape the image to (1, 48,48)\n",
    "def reshapeImagePixels(x):\n",
    "    x = np.array(x)\n",
    "    x = x/255\n",
    "    x = x.reshape(1, 48, 48)\n",
    "    return x\n",
    "\n",
    "def preprocess(data):\n",
    "    data['Pixels'] = data['Pixels'].apply(changeTypeToNumpyArray)\n",
    "    data['Pixels'] = data['Pixels'].apply(reshapeImagePixels)\n",
    "    processed_data = data[['Pixels', 'Emotion']]\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixels</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.8666666666666667, 0.9411764705882353, 0.9...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[0.39215686274509803, 0.4196078431372549, 0....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0.13725490196078433, 0.19607843137254902, 0...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[0.4666666666666667, 0.48627450980392156, 0....</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[0.6274509803921569, 0.6784313725490196, 0.7...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Pixels  Emotion\n",
       "0  [[[0.8666666666666667, 0.9411764705882353, 0.9...        3\n",
       "1  [[[0.39215686274509803, 0.4196078431372549, 0....        6\n",
       "2  [[[0.13725490196078433, 0.19607843137254902, 0...        4\n",
       "3  [[[0.4666666666666667, 0.48627450980392156, 0....        6\n",
       "4  [[[0.6274509803921569, 0.6784313725490196, 0.7...        2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Training Data\n",
    "train_data = preprocess(data)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[0.592156862745098, 0.592156862745098, 0.592...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[0.33725490196078434, 0.3176470588235294, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[0.1450980392156863, 0.23529411764705882, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[0.4549019607843137, 0.43137254901960786, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[0.396078431372549, 0.4470588235294118, 0.46...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Pixels\n",
       "0  [[[0.592156862745098, 0.592156862745098, 0.592...\n",
       "1  [[[0.33725490196078434, 0.3176470588235294, 0....\n",
       "2  [[[0.1450980392156863, 0.23529411764705882, 0....\n",
       "3  [[[0.4549019607843137, 0.43137254901960786, 0....\n",
       "4  [[[0.396078431372549, 0.4470588235294118, 0.46..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Test Data \n",
    "test_data = pd.read_csv('test.csv')\n",
    "test_data['Pixels'] = test_data['Pixels'].apply(changeTypeToNumpyArray)\n",
    "test_data['Pixels'] = test_data['Pixels'].apply(reshapeImagePixels)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to dataset format for pytorch use :\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Convert data to numpy array.\n",
    "        self.images = np.array(data['Pixels'])\n",
    "        self.labels = np.array(data['Emotion'])\n",
    "        # Convert to Tensors\n",
    "        print(self.images.shape)\n",
    "        print(self.labels.shape)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Get item at index location\n",
    "        img = self.images[index]\n",
    "        # Convert to tensor.\n",
    "        img = torch.from_numpy(img)\n",
    "        label = torch.from_numpy(self.labels)\n",
    "        label = label[index]\n",
    "        # return tesnor.    \n",
    "        return (img, label)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4178,)\n",
      "(4178,)\n"
     ]
    }
   ],
   "source": [
    "trainset = MyDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.8667, 0.9412, 0.9843,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.8784, 0.9333, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.8784, 0.9294, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.0510, 0.0627, 0.0431,  ..., 0.0706, 0.0667, 0.0863],\n",
       "          [0.0627, 0.0706, 0.0353,  ..., 0.0941, 0.0510, 0.0706],\n",
       "          [0.0627, 0.0745, 0.0314,  ..., 0.1686, 0.0745, 0.0627]]],\n",
       "        dtype=torch.float64), tensor(3))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a train loader for training :\n",
    "trainloader = DataLoader(trainset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)\n",
    "        self.max1 = nn.MaxPool2d(3, stride=2)\n",
    "        self.conv2 = nn.Conv2d(64, 64, 5)\n",
    "        self.max2 = nn.MaxPool2d(3,stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 4)\n",
    "        self.fc1 = nn.Linear(128 * 5 * 5 , 3072)\n",
    "        self.fc2 = nn.Linear(3072,7)\n",
    "        self.fc3 = nn.Softmax()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.max1(F.relu(self.conv1(x)))\n",
    "        x = self.max2(F.relu(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.dropout(x)\n",
    "        x = x.view(-1, 128*5*5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3200, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
      "  (fc3): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definign the loss and optimizer..\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adadelta(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  Loss on train set : 1.8881825454362475\n",
      "Epoch : 2  Loss on train set : 1.8787992055179508\n",
      "Epoch : 3  Loss on train set : 1.8727097838889553\n",
      "Epoch : 4  Loss on train set : 1.860624939430761\n",
      "Epoch : 5  Loss on train set : 1.7429446154878339\n",
      "Epoch : 6  Loss on train set : 1.7107250417461832\n",
      "Epoch : 7  Loss on train set : 1.6812073219823473\n",
      "Epoch : 8  Loss on train set : 1.6559555403148856\n",
      "Epoch : 9  Loss on train set : 1.6066202644173426\n",
      "Epoch : 10  Loss on train set : 1.5767704621526122\n",
      "Epoch : 11  Loss on train set : 1.5583376120065004\n",
      "Epoch : 12  Loss on train set : 1.552309807930284\n",
      "Epoch : 13  Loss on train set : 1.5432521441510616\n",
      "Epoch : 14  Loss on train set : 1.537078275025346\n",
      "Epoch : 15  Loss on train set : 1.5237068001550573\n",
      "Epoch : 16  Loss on train set : 1.5187154289420324\n",
      "Epoch : 17  Loss on train set : 1.5252550430880247\n",
      "Epoch : 18  Loss on train set : 1.520082546554449\n",
      "Epoch : 19  Loss on train set : 1.5230201546472448\n",
      "Epoch : 20  Loss on train set : 1.5174134232615697\n",
      "Epoch : 21  Loss on train set : 1.5191109926645991\n",
      "Epoch : 22  Loss on train set : 1.5114735610612475\n",
      "Epoch : 23  Loss on train set : 1.5023244610269562\n",
      "Epoch : 24  Loss on train set : 1.515383538399034\n",
      "Epoch : 25  Loss on train set : 1.505381460408218\n",
      "Epoch : 26  Loss on train set : 1.499536994759363\n",
      "Epoch : 27  Loss on train set : 1.5050581138552601\n",
      "Epoch : 28  Loss on train set : 1.497021624150167\n",
      "Epoch : 29  Loss on train set : 1.496361885361999\n",
      "Epoch : 30  Loss on train set : 1.4977018632961594\n",
      "Epoch : 31  Loss on train set : 1.4894126134974357\n",
      "Epoch : 32  Loss on train set : 1.4840954496660306\n",
      "Epoch : 33  Loss on train set : 1.4781393968421994\n",
      "Epoch : 34  Loss on train set : 1.4927096767279937\n",
      "Epoch : 35  Loss on train set : 1.4875135348953363\n",
      "Epoch : 36  Loss on train set : 1.4871219314691675\n",
      "Epoch : 37  Loss on train set : 1.479188991866949\n",
      "Epoch : 38  Loss on train set : 1.469528547680105\n",
      "Epoch : 39  Loss on train set : 1.4762316987714694\n",
      "Epoch : 40  Loss on train set : 1.4721906822146351\n",
      "Epoch : 41  Loss on train set : 1.468851919392593\n",
      "Epoch : 42  Loss on train set : 1.48542680813156\n",
      "Epoch : 43  Loss on train set : 1.471722551884542\n",
      "Epoch : 44  Loss on train set : 1.4653471735597567\n",
      "Epoch : 45  Loss on train set : 1.4675855563797113\n",
      "Epoch : 46  Loss on train set : 1.4668307704779937\n",
      "Epoch : 47  Loss on train set : 1.465342980304747\n",
      "Epoch : 48  Loss on train set : 1.4878569158888955\n",
      "Epoch : 49  Loss on train set : 1.4597299590365578\n",
      "Epoch : 50  Loss on train set : 1.4735908799499047\n",
      "Epoch : 51  Loss on train set : 1.4634493762300214\n",
      "Epoch : 52  Loss on train set : 1.4621837120929748\n",
      "Epoch : 53  Loss on train set : 1.4652799650002981\n",
      "Epoch : 54  Loss on train set : 1.4597715421487356\n",
      "Epoch : 55  Loss on train set : 1.4636173393890148\n",
      "Epoch : 56  Loss on train set : 1.4665518025405535\n",
      "Epoch : 57  Loss on train set : 1.4637630550005964\n",
      "Epoch : 58  Loss on train set : 1.4648543496168296\n",
      "Epoch : 59  Loss on train set : 1.4584969091051407\n",
      "Epoch : 60  Loss on train set : 1.4572133071549975\n",
      "Epoch : 61  Loss on train set : 1.4647928485433563\n",
      "Epoch : 62  Loss on train set : 1.4646627411587547\n",
      "Epoch : 63  Loss on train set : 1.459471374977636\n",
      "Epoch : 64  Loss on train set : 1.4605052452960998\n",
      "Epoch : 65  Loss on train set : 1.4646622752415315\n",
      "Epoch : 66  Loss on train set : 1.4569373676795085\n",
      "Epoch : 67  Loss on train set : 1.4558659910245706\n",
      "Epoch : 68  Loss on train set : 1.453418527850668\n",
      "Epoch : 69  Loss on train set : 1.457545506135198\n",
      "Epoch : 70  Loss on train set : 1.4673722827707538\n",
      "Epoch : 71  Loss on train set : 1.4591752845822399\n",
      "Epoch : 72  Loss on train set : 1.4551596605140744\n",
      "Epoch : 73  Loss on train set : 1.4610269561068703\n",
      "Epoch : 74  Loss on train set : 1.4533420009467437\n",
      "Epoch : 75  Loss on train set : 1.4488178282293656\n",
      "Epoch : 76  Loss on train set : 1.4647420635660187\n",
      "Epoch : 77  Loss on train set : 1.445409294303137\n",
      "Epoch : 78  Loss on train set : 1.4577786977054508\n",
      "Epoch : 79  Loss on train set : 1.4444046602904341\n",
      "Epoch : 80  Loss on train set : 1.4471086108957538\n",
      "Epoch : 81  Loss on train set : 1.45945704802302\n",
      "Epoch : 82  Loss on train set : 1.4531657677570373\n",
      "Epoch : 83  Loss on train set : 1.449942785364981\n",
      "Epoch : 84  Loss on train set : 1.4481034606467678\n",
      "Epoch : 85  Loss on train set : 1.4445103070208134\n",
      "Epoch : 86  Loss on train set : 1.4421564932087905\n",
      "Epoch : 87  Loss on train set : 1.4424744817136808\n",
      "Epoch : 88  Loss on train set : 1.4508707759944537\n",
      "Epoch : 89  Loss on train set : 1.457211792924022\n",
      "Epoch : 90  Loss on train set : 1.4459803923395753\n",
      "Epoch : 91  Loss on train set : 1.4511891139372615\n",
      "Epoch : 92  Loss on train set : 1.444874304851503\n",
      "Epoch : 93  Loss on train set : 1.4379980655116888\n",
      "Epoch : 94  Loss on train set : 1.446260525070074\n",
      "Epoch : 95  Loss on train set : 1.4472196156742008\n",
      "Epoch : 96  Loss on train set : 1.4430364943642653\n",
      "Epoch : 97  Loss on train set : 1.4440664043863312\n",
      "Epoch : 98  Loss on train set : 1.4370001872987237\n",
      "Epoch : 99  Loss on train set : 1.4490977280012525\n",
      "Epoch : 100  Loss on train set : 1.4471527565526598\n"
     ]
    }
   ],
   "source": [
    "# Trainign the model\n",
    "nb_epochs = 100\n",
    "model.train()\n",
    "for epoch in range(1, nb_epochs+1):\n",
    "    train_loss = 0\n",
    "    i = 0\n",
    "    for images,label in trainloader:\n",
    "        images = images.cuda().float()\n",
    "        label = label.cuda()\n",
    "        #images = images.float()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss\n",
    "        i = i + 1\n",
    "    print(\"Epoch :\", epoch, \" Loss on train set :\", train_loss.item()/(i*1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "img,lab = next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "output = model(img.cuda().float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = torch.argmax(output, dim=1)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = output.cpu().numpy()\n",
    "lab = lab.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.625"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Overall accuracy :\n",
    "cor = sum(output == lab)\n",
    "cor/32*100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_save.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_save_state.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (max2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=3200, out_features=3072, bias=True)\n",
      "  (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
      "  (fc3): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (max1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (max2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=3200, out_features=3072, bias=True)\n",
       "  (fc2): Linear(in_features=3072, out_features=7, bias=True)\n",
       "  (fc3): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.load_state_dict(torch.load('model_save_state.pt'))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using opencv :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "faceCascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt2.xml\")\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "target = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = faceCascade.detectMultiScale(gray, scaleFactor=1.1)\n",
    "\n",
    "    # Draw a rectangle around the faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2, 5)\n",
    "        face_crop = frame[y:y + h, x:x + w]\n",
    "        face_crop = cv2.resize(face_crop, (48, 48))\n",
    "        face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "        face_crop = face_crop.astype('float32') / 255\n",
    "        face_crop = face_crop.reshape(1, 1, face_crop.shape[0], face_crop.shape[1])\n",
    "        \n",
    "        face_crop = torch.from_numpy(face_crop)\n",
    "        output = model(face_crop)\n",
    "        output = torch.argmax(output, dim=1).numpy()\n",
    "        \n",
    "        result = target[int(output)]\n",
    "        cv2.putText(frame, result, (x, y), font, 1, (200, 0, 0), 3, cv2.LINE_AA)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
